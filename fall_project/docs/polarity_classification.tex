The final part of the analysis is the polarity classification (positive/negative). While TSA is not yet considered mature, SA for longer texts, i.e., documents and reviews, has been explored for years~\citep{book:pang}. Different techniques and algorithms that have proven worthy for longer texts have also been applied to sentence level SA with various success. Among these techniques, supervised learning methods like naive Bayes classifier (NB), maximum entropy (MaxEnt) and support vector machines (SVM) are the most used. The limited amount of attributes in tweets makes the feature vectors shorter than in documents. For that reason there is no guarantee that algorithms that perform well on document level SA will be the best alternatives for classifying short texts like tweets.

Some approaches have also experimented with a combination of lexicon-based methods and  machine learning~\citep{article:mudinas}. They perform an entity-level sentiment analysis as the first step. Then they use tweets that are likely to be opinionated in a lexicon-based method. The last step of their process is to train a classifier to assign the sentiment value. This approach makes it possible to train the classifier without manually labeling the data, as they are using the data from the lexicon-based method.

\cite{article:omg} tried different solutions for the polarity classification, and found that the best performance came from using n-grams feature selection together with lexicon features and microblog features. Interestingly, the performance dropped when they included a POS tagger. They do not explain the reason for this, but speculate that it can be the accuracy of the POS tagger itself, or just that POS tagging is less effective for analyzing the polarity of tweets.\vspace{8mm}

\noindent
\textbf{Supervised learning} \\
\noindent
Supervised learning methods require some sort of training data to create an inferred function for classification tasks. These data would preferably be manually annotated texts, but as this can be a labor-intensive task, some research has experimented with emoticons or a collection of hashtags as labels for positive/negative tweets. This is done by making assumptions, such as that all tweets containing positive emoticons are positive, and that all who contain negative emoticons are negative.

Among the machine learning algorithms that perform well on TSA are NB, SVM and MaxEnt. While the SVM technique normally beats NB and MaxEnt on longer texts, it seems to have some trouble with outperforming the NB when feature vectors are shorter, i.e., on shorter texts. \cite{article:bermingham} have shown this in their comparison of SVM and NB for microblogs.\vspace{8 mm}

\noindent
\textbf{Unsupervised learning} \\
\noindent 

The lexicon-based method seems to be the most used approach of the unsupervised methods in TSA. This technique requires a lexicon with a sentiment score for each word. When using such lexica the classifier can look up all the words in the feature vector, e.g., a bag of words, and check the sentiment score if the feature exists in the lexicon. Hence it will not need any training beforehand.
	
Popular sentiment lexica are SentiWordNet and the General Inquirer. Some researchers have also made custom extensions of these lexica that include manually annotated emoticons and hashtags as well as words. \cite{article:afinn} made a sentiment lexicon called AFINN, specialized for Twitter. It contains a lot of words from the vocabulary used in social networks. AFINN supports slang and abbreviations, e.g., ‘n00b’, ‘lol’ and ‘wtf’. This lexicon was made as a response to the ANEW lexicon which works better for document level SA since it does not support the Twitter language.