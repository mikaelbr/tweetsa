\chapter{State of the Art}

In this section it is described how a systematic literature review (\nom{SLR}{Systematic Literature Review}) was conducted to establish the state of the art of a Twitter Sentiment Analysis (\nom{TSA}{Twitter Sentiment Analysis}) system, and the result of the review. The first section, Section~\ref{sec:slrintro}, covers the introduction and defines our research questions. The review method is described in Section~\ref{sec:slrmethod} and the result in Section~\ref{sec:slrresults}.

\section{Introduction}
\label{sec:slrintro}

SLRs are still new in the field of computer science. There are few examples of an SLR in practice. The method we used is heavily inspired by the documentation paper by~\cite{paper:slrdesc} and the Master's thesis by~\cite{master:slr}. \\

\noindent We defined our research questions (\nom{RQ}{Research Question}) as the following:

\begin{description}

\item[RQ1] What are some of the existing solutions for SA (sentiment analysis) in the Twitter Corpus.
\item[RQ2] How does the different solutions found by addressing RQ1 compare to each other with respect to micro-blogs like Twitter.
\item[RQ3] What is the strength of the evidence in support of the different solutions?
\item[RQ4] What implications will these findings have when creating the application/system?

\end{description}

\section{Review method}
\label{sec:slrmethod}

In this section we will describe our process step by step according to the systematic literature review. 

\subsection{Planning}

\begin{description}

	\item[1. Identification of the need for a review] \hfill \\
		Assumed that a review is needed for this project. 
		
	\item[2. Commissioning a review] \hfill \\
		Assumed to be commissioned for this project, so no commission report was produced. 

	\item[3. Specifying the research question(s)] \hfill \\
		We defined the RQs based on what we felt needed to be researched when finding the state-of-the-art for sentiment analysis systems on Twitter. The RQs can be seen in Section~\ref{sec:slrintro}. 

	\item[4. Developing a review protocol] \hfill \\
		The review protocol \nom{SLRP}{Systematic Literature Review Protocol} was developed in the early stages of the project. After the first revision it was evaluated by the project supervisor. The protocol was revised several times during the execution of the review.
	

	\item[5. Evaluating the review protocol] \hfill \\
		The protocol was evaluated by the project supervisor. 

\end{description}


\subsection{Conducting}

\begin{description}

	\item[1. Identification of research] \hfill \\
		We defined a series of keywords and synonyms to construct a search string to use. The search string was defined to find papers relevant to our research questions. The development of this search string is documented by Appendix~\ref{apx:slrp}. The search string we used was:
		
		\begin{verbatim}
		("Sentiment Analysis" OR "Sentiment Classification" 
		OR "Opinion Mining") AND (Twitter OR Microblog)
		\end{verbatim}
		
		For the search domain, we used Google Scholar. Google Scholar accumulates results from several different sources and gave many results for our search. A lot of the results given corresponded to the studies handed out by the project supervisor, a domain expert. 
		
		We limited the search to only give papers released after 2008. This is due to Twitter being as new as it is.
		
		The search resulted in 1060 papers, but after the first 8 pages of results (with 10 papers per page), we found that the papers were mostly irrelevant and we had limited resources and time for handling all 1060 papers. This resulted in a set of 80 papers, ready to go through the selection process.
		
		

	\item[2. Selection of primary studies] \hfill \\
		First, we defined a set of primary inclusion criteria. These criteria were applied to the title and abstract of the study. If a paper did not pass the criteria, it was dropped from the set. Secondly we defined secondary inclusion criteria. These criteria were checked against the full text of the study. These inclusion criteria are documented in the protocol in~\autoref{apx:slrp}. 
		
		After both selection processes, we had a set of 23 papers. 

	\item[3. Study quality assessment] \hfill \\
		With the help of \cite{paper:slrdesc} we defined a set of 10 quality criteria. All of the criteria are documented in the protocol in~\autoref{apx:slrp}. Each of the papers in our set was assessed with all of these criteria. If the paper met the criteria, it would get 1 point, if met partly it would get 0,5 points, and if it did not meet the criteria it would get 0 points. 
		
		The papers with the lowest score did not get taken as much into consideration when defining the state-of-the-art. 
		
	\item[4. Data extraction and monitoring] \hfill \\
		We defined a set of fields and information categories we thought were important in order to answer our research questions. These data features were used to generate a table of information. The information was retrieved by reading the papers and manually extracting the data we needed.
	

	\item[5. Data synthesis] \hfill \\
		This step was included in the data extraction step.
\end{description}

% This probably shouldn't be here but in the SLR chapter..
% For this report, the data synthesis is included as a part of the data extraction. 

\subsection{Reporting}


\begin{description}

	\item[1. Specifying dissemination strategy] \hfill \\
		As this is for a master thesis, the result of the SLR is presented in this report.  

	\item[2. Formatting the main report] \hfill \\
		The SLR was written as a section in this project report. 

	\item[3. Evaluating the report] \hfill \\
		It is mandatory for an expert to review this report as well as a project supervisor, as it is a report for a master thesis.

\end{description}

\section{Results}
\label{sec:slrresults}

In this section the result of the systematic literature review is presented. In Section~\ref{sec:selected} all of the extracted data from the selected studies are presented. The assessed quality is also presented. 

In Section~\ref{sec:stateofart} the answers to our research questions are given. 

\subsection{Selected studies}
\label{sec:selected}

In this section all of the selected studies are presented in table format as a part of the data extraction step in the SLR. The results can be found in~\autoref{tab:extraction}. In addition to the data features defined in the review protocol, the total quality is added to be a part of the extraction table.

\input{2-state_of_the_art/slr/extractiontable}

In \autoref{tab:quality} all the individual points for the quality criteria are presented. The criteria are defined as a part of the review protocol. The average criterion score was 8.0.

\input{2-state_of_the_art/slr/qualitytable}

\subsection{Twitter Sentiment Analysis: State-of-the-Art}
\label{sec:stateofart}
\input{2-state_of_the_art/tsa}

\subsection{State-of-the-Art Post SemEval'13}

The SLR was performed before the SemEval'13 tasks~\citep{WilsonEA:13}. This means that a lot of work has been done on TSA after the state-of-the-art was defined. Many of the points are still relevant, but some additional information can be extracted. 

In relation to SemEval'13,~\cite{MohammadEA:13} defined and implemented a state-of-the-art system. The system implemented by~\cite{MohammadEA:13}, is described as the strongest system by~\cite{WilsonEA:13}, showing the highest F-measure on both tweets and SMS.

\subsubsection{Sentiment Lexicon}

The system developed by~\cite{MohammadEA:13}, used lexica to help classify sentiment. In addition to using existing general lexica like the NRC Emotion Lexicon~\citep{mohammad2010emotions, mohammad2011tracking}, MPQA Lexicon~\citep{wilson2005recognizing}, and the Bing Liu Lexicon~\citep{hu2004mining}, they also used Twitter specific lexica. One Twitter specific lexicon was developed for the system by streaming from the Twitter API from April to December 2012, using 78 different seed words as hash tags~\citep{MohammadEA:13}. In addition to their own lexicon, the Sentiment140 by~\cite{article:go} lexicon was used.

\subsubsection{Classifier and Features}

As with many previous systems (e.g. \cite{article:bermingham}, \cite{chamlertwat2012discovering}, \cite{zhang2011combining}, \cite{asiaee2012if}), supervised learning with SVM was used on the training data provided by SemEval'13. The classification was done in one step, classifying three ways (neutral, positive and negative)~\citep{MohammadEA:13}.

URLs and user names were normalised to placeholder text (\textit{http://someurl} and  $@$$someuser$) and the tweets were tokenized and part-of-speech tagged using a tool for doing natural language processing on Twitter by~\cite{gimpel2010part}. 

The features used were word ngrams, character ngrams, number of words in all caps, POS tags, hashtags, lexica, punctuations, emoticons, the number of elongated words (e.g. $goooood$), clusters and the number of negated contexts. \cite{MohammadEA:13} found that using all these features gave the best performance for F-measure.