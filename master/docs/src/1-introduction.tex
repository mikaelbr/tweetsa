\chapter{Introduction}

In this chapter an introduction of this Master's thesis is given. First the task description is stated. Background and Motivation for this task is given in~\autoref{sec:motivation}, followed by an overview of the project goals in~\autoref{sec:projectgoals}. In~\autoref{sec:design} the research design is outlined and in~\autoref{sec:contribution} the project contribution is summarized. In the last section of the introduction, an overview of this report is described. 

This report is the result of a Master thesis at Department of Computer and Information Science, NTNU, 2013. 

\section{Task Description}
\label{sec:task}

The task was given by Bj\"{o}rn Gamb\"{a}ck at IDI, NTNU

\begin{center} \Large Sentiment Analysis using the Twitter Corpus \end{center}
\begin{quotation}
In recent years, micro-blogging has become prevalent, and the Twitter API allows users to collect a corpus from their micro-blogosphere. The posts, named tweets are limited to 140 characters, and are often used to express positive or negative emotions to a person or product.

In this project, the goal is to use the Twitter corpus to do sentiment analysis and develop tools for visualizing the results. Pak and Paroubek (2010) have shown how to do this using frameworks like Support Vector Machines (SVMs) and Conditional Random Fields (CRFs), benchmarked with a Naive Bayes Classifier baseline. They were unable to beat the baseline, and the goal of this project will be to experiment with these and other machine learning frameworks as Maximum Entropy learners to try to beat the baseline.
\end{quotation}


\section{Background and Motivation}
\label{sec:motivation}

Twitter has become a popular social media service often referred to as a micro-blogging site. On Twitter users can post messages of maximum 140 characters, called tweets, on their own \emph{timeline}. A timeline is a collection of all user submitted tweets and all tweets from the other users that a user is connected to (following). Tweets can be categorized by using hashtags. A hashtag can, for instance, look like \emph{\#happy} or \emph{\#obama2012}. By annotating the tweets with this tag, users can find similar tweets across Twitter. Tweets often also contain references to other users by using the $@$-character followed by the user name (e.g. \emph{$@$obama}) or references to pictures or articles via URLs.

Twitter has grown very rapidly and the usage statistics is ever changing. In June 2012, there were posted over 400 million tweets every day~\citep{site:twitterusage}. With over 500 million users, where about 170 million of these are active ones~\citep{site:users}, it is safe to say that Twitter offers a lot of data.

The informal nature of Twitter leads to a lot of sentiments being posted and this has led Twitter to being a gold mine for SA. Many systems have used Twitter as corpus for sentiment analysis. The first one to really use Twitter as a corpus was Sentiment140 (previously known as TwitterSentiment) by a group of Stanford students~\citep{article:go}. After this paper, and as Twitter grew in popularity, many other systems have been developed. Some of the later ones are TwiSent and C-Feel-IT~\citep{mukherjee2012twisent}, Tweenator~\citep{saif2012semantic} and MSAS~\citep{chamlertwat2012discovering}.

Many system uses a single step, three way, classification for sentiments. But another approach is to do a two step classification, where firstly the subjectivity is classified and in step two, the polarity. Subjectivity classification is the task of classifying if a tweet is subjective or objective. \cite{article:pak} counted word frequencies in a subjective vs an objective set of tweets; the results showed that interjections and personal pronouns are the strongest indicators of subjectivity. If the tweet is classified as objective (or neutral), no further classification is required. If the tweet is subjective, how ever, it requires polarity classification. Polarity classification, will classify between positive and negative tweets.~\cite{article:omg} experimented with different solutions for tweet polarity classification, and found that the best performance came from using n-grams together with lexicon and micro-blog features. Interestingly, performance dropped when a part-of-speech (POS) tagger was included. They speculate that this can be due to the accuracy of the POS tagger itself, or that POS tagging just is less effective for analysing tweet polarity.

The growth in Twitter users and status updates (tweets) over the last years has made Twitter an attractive platform for companies, marketeers, politicians and others who are looking for feedback. Manually collecting information like this is a tedious if not impossible task.

The informal texts on social media represent challenges for traditional natural language processing systems. These texts are short, and often con- tain misspellings, slang and abbreviations. The challenge of handling such a vocabulary has only been researched over the last few years.

Another interesting feature is that Twitter messages offer a lot of meta data and information about their origin, such as location, language, and more. These data could for example be used to filter out and classify tweets from a certain event, like a festival or a conference.

For visualising sentiment data, not as much has been done. Sentiment140 has some rudimentary visualising with pie and bar charts\footnote{\url{http://www.sentiment140.com/}}. \cite{article:wefeelfine} developed an emotional search engine with different advanced techniques for visualising mood, but not for three way classification for sentiment on Twitter.

\section{Project Goals}
\label{sec:projectgoals}

In this section, the main goals of for this project are described.

\begin{description}

\item[G1] \textbf{Experiment with different methods for doing sentiment analysis} \\
	Design and implement different methods for doing sentiment analysis. Experiment with these methods and find the method with highest accuracy and beating the baseline the most. 
	
\item[G2] \textbf{Develop tools for visualizing sentiment classified tweets} \\
    Data is wasted if it is not used for anything. Data needs proper, usable, summarizing and visualization tools to be of use. One of the goals of this project is to come up with, plan and develop tools that are useful for showing the real value of sentiment classified tweets. 

\end{description}

	

\section{Contributions}
\label{sec:contribution}

\begin{itemize}
\item[\textbf{C1}] This thesis provides a definition of the state-of-the-art for Twitter Sentiment Analysis.

\item[\textbf{C2}] A general system architecture for doing Twitter Sentiment Analysis is implemented. 

\item[\textbf{C3}] Different machine learning algorithms are compared for the task of identifying sentiments in short messages in a fairly semi-independent domain.

\item[\textbf{C4}] Visualization applications are implemented, showing how to use data from the generic system and examples of how to show sentiment analysis data.
 
\end{itemize}

\section{Thesis Structure}
\label{sec:structure}

In Chapter 2, the existing solutions and current state of the art is described. In Chapter 3, data sets and machine learning theory is described. The system architecture and model is presented and documented in Chapter 4. Chapter 4 also has a complete description of visualisation application architecture and how the applications work. In Chapter 5 experiments and results regarding different approaches for doing Sentiment Analysis is described. Chapter 5 also describes the results for the developed visualisation applications. Chapter 6 includes evaluation and discussions of the results. In Chapter 7 the report is concluded and future work is described. 

% \section{Examples REMOVEME!}

% This is an example of a reference \cite{copland2000}.

%  This is a reference to Table \ref{tab:1-relations} and to Figure \ref{fig:1-studies_contributions_papers}.

% A glossary example which will be included in the glossary at the end of the document:
% \glossary{name=Newton,description=Unit of force but may also refer to Sir Isaac Newton.}

% An abbrevation example which will be included in the list of abbrevations: 
% \abbr{name=NTNU,description=Norwegian University of Science and Technology}

% To use the glossary:
% 1. Compile the latex document.
% 2. Run the makeGlossary.{bat|sh} (.bat for windows, .sh for Linux/Unix). This script ASSUMES that your latex file is
% called document.tex 
% 3. Compile the latex document again (I have sometimes experienced some error messages but as long as I compile a
% couple of times it is ok)
