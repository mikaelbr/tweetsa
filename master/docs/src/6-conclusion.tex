\chapter{Conclusion}

We found that machine learning algorithms perform well for doing domain semi-independent classification for sentiments. We tested three different base machine learning algorithms: Naive Bayes, SVM and MaxEnt, in addition of combining them for doing two-step classification and boosting with voting for sentiment. 

Experiments show that SVM and MaxEnt in a single step classification perform the best for these datasets. Given the confusion matrices and results on different data sets, it seems that SVM performs better than MaxEnt, but having a neutral-heavy development set, caused MaxEnt to perform better than SVM while doing a complete full grid search across multiple values.

Reducing the training set to normalize number of tweets per label did not improve the overall performance in accuracy but improved the F1-score and shows more positive trending confusion matrices. 

The architecture purposed for this system performs well on both regular REST API requests, but is also capable to do streaming of tweets and classify tweets in real-time for several tweets per second. By extending the Twitter API and having the same end point interface, no additional documentation is needed for the API and existing libraries and programming language specific API wrappers can be used to retrieve information from our API Layer by only changing their URL connection point. 

\section{Contribution}

In this project we have defined the state of the art for Twitter Sentiment Analysis systems, and implemented a generic system with an architecture capable of classifying many tweets per second in a live stream. We found that, by extending the Twitter API and just attaching a value for sentiments, the data can easily be used to implement visualisation applications, or extending existing Twitter based systems. 

We have tested different machine learning algorithms, reported to work well with TSA, and found that SVM and MaxEnt performs well on domain semi-independent short messages. 

\section{Future Work}

A natural way to extend this work, is to add additional classification algorithms as models. There are also several different features that could be investigated and more combinations of pre-processing methods. 

To improve the classification on tweets, and thus make the system less domain independent, more Twitter specific features can be used. E.g. lexica developed specifically for social media, like the~\citet{article:afinn} lexicon. 

It could also be interesting adding POS-tagging as an experiment, using it to classify subjectivity as shown to work by~\citet{article:pak}.

While developing the visualisation application SentiMap, another more specific feature could be seen in the tweets streamed by Twitter. As the tweets were restricted by geographical location, most of them originated from hand held devices. Apples iPhone, has their own smilies, originally intended for the Japan user marked, called Emoji\footnote{\url{http://en.wikipedia.org/wiki/Emoji}}. Emoji smilies are different smilies and icons representing situations of sentiments, but not constructed from ASCII characters like regular smilies (e.g. $:($ and $:)$). By translating these Emojies to sentiments like $||Happy||$ or $||Sad||$ they could be used as features.