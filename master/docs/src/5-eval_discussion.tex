\chapter{Discussion}

In the introduction two main goals were introduced for this Master's Thesis:

\begin{description}

\item[G1] \textbf{Experiment with different models for doing sentiment analysis}
	
\item[G2] \textbf{Develop tools for visualising sentiment classified tweets}

\end{description}

This chapter will discuss whether or not we succeeded in reaching these goals, and discuss the solutions in general. The first section handles G1 and the second section discusses G2. The generic system architecture is an important part of achieving making the visualisation applications, and thus it is included in the latter section.

\section{G1: Experiment with different models for doing sentiment analysis}

In this section we will discuss whether we succeeded with our first goal or not, and how it was conducted. The first goal stated that we should try different models for classifying sentiment on short messages, such as tweets. 

The experiment description from~section~\ref{sec:experiment} show that we used grid searching on different machine learning algorithms and combination of algorithms to classify sentiment. A total set of 13 different models were thoroughly tested with a large training set. Our system generated graphs and plots comparing the different models, both on accuracy, F1-score, recall, precision and their confusion matrices. This comparative view allowed us to find what model performed best.

In our experiments we found that MaxEnt and SVM perform best in regards to accuracy. By conducting the experiments as stated, and finding the models with the highest accuracy, we succeed with the first goal.

The data that was used to train and evaluate the different models was given by SemSeval'13, who also provided a test set with SMS messages for the second part of the task. We considered this part of the task an opportunity to test if the system really was domain semi-independent. The results from the task indicates that the system performed well both for the constrained and the unconstrained versions. The system ranked 5th of 28 constrained systems and 6th of 15 unconstrained systems, which is an encouraging result in terms of domain semi-independence. A part of the strategy to obtain good classifications on several domains was to use general feature selection methods. When looking at our feature selection for the SMS constrained, we see that no Twitter specific features are used. User names, hash-tags as words, URLs, RT-tags, etc, are removed. What we have left is essentially the same content as a SMS would have; text with emoticons. For the unconstrained system, some Twitter specific features is included, as hash-tags, RT-tags and URLs, but even with these features a tweet are similar to an SMS message. Both tweets and SMS messages are short (respectively 140 and ~160 characters), and contain similar language. 

Some of the preprocessing methods are to be considered naive, such as the negation handling. In these experiments we only utilized unigram feature selection with a simple negation support. This simple method binds all appearances of the word 'not' to the next and previous word in the sentence, e.g., 'is not bad' would be merged into the unigram ['is-not', 'not-bad']. This is a very naive approach which in some cases may extend the vocabulary with little informative features. Rather than the simple treatment of negation used here, an approach to automatic induction of scope through a negation detector~\citep{CouncillEA:10} could be used.

From the list of features we can see that emoticons are informative features, both for SVM and MaxEnt. However, we did not use any placeholders to normalize these emoticons. By using placeholders, the emoticons that are not frequently used would gain information. Another potential improvement is support for Emoji, wich has very similar semantics as emoticons, but are implemented in Apple products as an own character set (rather than constructed from ascii characters such as ':)' and ':D', etc). There are reasons to believe that these Emojis would provide informative features to the vocabulary, and could either be converted to ascii characters or placeholders to merge them with the emoticons.

While the focus for this goal was to experiment with different models for SA, there are still some interesting approaches that remain unexplored. 

The data sets used to train our models was not evenly distributed among the different target classes. This may have affected the results for some of the algorithms that were used. The data has a large amount of neutral tweets, which seemed to favor especially MaxEnt when classifying neutral tweets. To even out the distribution in the data set, we tried to limit the number of tweets per class. The results when limiting to 2000 tweets per class gave better recall and F1-measure, but a small decrease in both accuracy and precision. The confusion matrix also indicate better ability to successfully classify negative instances. This was an expected result since the classifier was trained on a more balanced data set. But the data set was still missing some (around 800) negative training instances to be perfectly balanced. The lack of accuracy and precision may be a consequence of lacking training data across all classes. This theory is backed up by the decreasing performance when limiting to 1000 tweets per class, but the classification of negative tweets is better. 


\section{G2: Develop tools for visualising sentiment classified tweets}

We achieved goal 2 by implementing two different applications for visualising sentiment analysis data, and by designing and using a generic system for classifying tweets. 

We are satisfied with the way the generic system architecture is built. By just extending the Twitter API, no API documentation is needed, and if you are familiar with the Twitter API, you don't have to re-learn anything. In addition, if there is a system all ready integrated with Twitter data, the migration to our system is simple; swap the entry URL point from the Twitter API base URL to our system base URL. 

The current implementation of the API Layer is simplified in regards to authentication. The API Layer has no OAuth server, but rather uses its own credentials to connect to Twitter. This means, if 10 different users do 30 requests each, Twitter's request limit will engage and our API Layer will be put on hold until next window of requests. A better solution would have been to implement a mirror of Twitters OAuth server and pass on the received credentials to Twitter. This way each end-user or application client would have their own pool of requests. 

One important point when designing the generic system was to have it as fast as possible. This to be able to handle large amount of tweets when streaming or simply searching with a high count limit. In some cases a client can request 1000 tweets at once. This required a system that can operate in parallel and handle asynchronous connections. The way our system was built, the API layer works independent of the classification server and each request is parallel and asynchronous. This maximize the number of tweets we can handle and reduce the collected wait time for the client. This solution works great for the applications we have built and for up to 5 clients running simultaneously, but the system has not been tested with any more clients or stress-tested in any way. A stress-test might show a bottleneck or a weakness in the system that is not apparent at this point. 

\subsection{SentiMap}

The SentiMap application show interesting information and distribution of tweets a cross the USA. It is capable of handling at least 50 tweets per second, and updates the map's colour scheme for each tweet. Every tweet is grouped by state, so the system looses some information in regards to locality. It could have been interesting to add more details to the map, showing the exact origin of a Tweet in addition to it changing the state sentiment indication colour.

When opening SentiMap now, you start out from scratch. There is no history or storage for the tweets. If you were to open SentiMap and have it classify 1000 tweets, refreshing the application will remove these 1000 tweets. Adding a local storage for these tweets, could provide some usefulness. Also concatenating search data with stream data could be useful, starting the application with some initial data. If some big event happens now, a user has to be quick to open SentiMap to see the sentimental development. 

There is no automatic way of plugging in a different country to the SentiMap application, but the architecture allows for easy system extension. By having defined a clear MV* architecture, each module is fairly independent of each other, and can thus be replaced with different modules. To make this even easier, a plug-in system could have been designed and documented. 

By using WebSockets to provide data from the API Layer, a continuously connection is established. This means that if the API Server goes down, or the visualisation application back-end server restarts, the application will automatically reconnect without any user action. E.g. if you open SentiMap on a laptop, closes this laptop, and then re-open it, SentiMap will reconnect and start showing data again. This means you can have the client open over a long, long time, if necessary.   

\subsection{SentiGraph}
SentiGraph was developed as a light weight JavaScript application that runs entirely in the client's web browser. This makes the application both fast, and compatible with most platforms. It demonstrates how the API Layer (and the Twitter API) can be used to search for different topics or keywords. It would, however, be possible to combine this with a streaming service. But that would require a small server side application to push the incoming tweets to the client, and thus breaking the concept of having the entire application running in the browser.

SentiGraph is a good application for visualising the sentiment data that are generated by the sentiment classifier. But there are definitely room for more features, such as comparison of keywords and, of course, several different graphs and charts. A graph that showed changes in sentiment over time could be a useful tool to visualise trends, and possibly changes caused by certain events or happenings. 



%\textcolor{blue}{(8-10 pages) In this chapter you assess your results. Identify your contributions. Possible theory 
%building (establish cause-effect). Compare to other work described in chapter 2. Suggestions for improvements. Discuss 
%construct-, internal-, external- and conclusion-validity. The major challenge in this chapter is usually which axis you 
%want to structure your discussion around: research questions, contributions or studies. Find what works best for you 
%and your studies.}
%
%\textcolor{green}{Evaluation of research questions}
%
%\textcolor{blue}{If you did not answer these questions in the results chapter, now is the time to revisit.}
%
%\textcolor{green}{Evaluation of Contributions}
%
%\textcolor{blue}{How does our contributions fit with the state of the art we described in chapter 2? Do they extend the 
%field? In what way? How do your contributions compare to your research questions? Do you have your own reflections on 
%the contributions.}
%
%\textcolor{green}{Evaluation of Validity Threats}
%
%\textcolor{blue}{What are the major threats to our research? Mention the major threats like:}
%
%\begin{list}{$\bullet$}{}
%  \item Internal Validity 
%  \item External Validity
%  \item Construct Validity
%  \item Conclusion Validity
%\end{list}
%
%\textcolor{blue}{Note that you might have to discuss these separately for each study, and every validity might not be applicable depending on what research method you have used.}
%
%\textcolor{green}{Reflections on the research context}
%
%\textcolor{blue}{Optional. But it is often good to reflect on the (project) context of your research and how it has 
%affected you and your research.}