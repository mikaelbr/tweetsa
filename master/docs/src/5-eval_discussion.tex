\chapter{Discussion}

In the introduction two main goals were introduced for this Master's Thesis:

\begin{description}

\item[G1] \textbf{Experiment with different models for doing sentiment analysis}
	
\item[G2] \textbf{Develop tools for visualising sentiment classified tweets}

\end{description}

This chapter will discuss whether or not we succeeded in reaching these goals, and discuss the solutions in general. The first section handles G1 and the second section discusses G2. The generic system architecture is an important part of developing the visualisation applications, and thus it is included in the latter section.

\section{G1: Experiment with different models for doing sentiment analysis}

In this section we will discuss whether we succeeded with our first goal or not, and how it was conducted. The first goal stated that we should try different models for classifying sentiment on short messages, such as tweets. 

The experiment description from~section~\ref{sec:experiment} show that we used grid searching on different machine learning algorithms and combination of algorithms to classify sentiment. A total set of 13 different models were thoroughly tested with a large training set. Our system generated graphs and plots comparing the different models, both on accuracy, F1-score, recall, precision and their confusion matrices. This comparative view allowed us to find what model performed best.

In our experiments we found that MaxEnt and SVM perform best in regards to accuracy. By conducting the experiments as stated, and finding the models with the highest accuracy, we succeed with the first goal.

The data that was used to train and evaluate the different models was given by SemSeval'13, who also provided a test set with SMS messages for the second part of the task. We considered this part of the task an opportunity to test if the system really was domain semi-independent. The results from the task indicates that the system performed well both for the constrained and the unconstrained versions. The system ranked 5th of 28 constrained systems and 6th of 15 unconstrained systems, which is an encouraging result in terms of domain semi-independence. A part of the strategy to obtain good classifications on several domains was to use general feature selection methods. When looking at our feature selection for the SMS constrained, we see that no Twitter specific features are used. User names, hash-tags as words, URLs, RT-tags, etc, are removed. What we have left is essentially the same content as a SMS would have; text with emoticons. For the unconstrained system, some Twitter specific features is included, as hash-tags, RT-tags and URLs, but even with these features a tweet are similar to an SMS message. Both tweets and SMS messages are short (respectively 140 and ~160 characters), and contain similar language. 

Some of the preprocessing methods are to be considered na\"{i}ve, such as the negation handling. In these experiments we only utilized unigram feature selection with a simple negation support. This simple method binds all appearances of the word 'not' to the next and previous word in the sentence, e.g., 'is not bad' would be merged into the unigram ['is-not', 'not-bad']. This is a very na\"{i}ve approach which in some cases may extend the vocabulary with little informative features.

From the listed features, emoticons are among the most informative, both for SVM and MaxEnt. However, we did not use any placeholders to normalize these emoticons. By using placeholders, the emoticons that are not frequently used would gain information. Another potential improvement is support for Emoji\footnote{\url{http://en.wikipedia.org/wiki/Emoji}}, wich has very similar semantics as emoticons, but are implemented in Apple products as an own character set (rather than constructed from ascii characters such as ':)' and ':D', etc). There are reasons to believe that these Emojis would provide informative features, and could either be converted to ascii characters or placeholders to merge them with the emoticons. When not supported, the Emojis would show up as ascii squares: '$\square$'

While the focus for this goal was to experiment with different models for SA, there are still some interesting approaches that remain unexplored. 

The data sets used to train our models was not evenly distributed among the different target classes. This may have affected the results for some of the algorithms that were used. The data has a large amount of neutral tweets, which seemed to favor especially MaxEnt when classifying neutral tweets. To even out the distribution in the data set, we tried to limit the number of tweets per class. The results when limiting to 2000 tweets per class gave better recall and F1-measure, but a small decrease in both accuracy and precision. The confusion matrix also indicate better ability to successfully classify negative instances. This was an expected result since the classifier was trained on a more balanced data set. But the data set was still missing some (about 800) negative training instances to be perfectly balanced. The decreased accuracy and precision may be a consequence of lacking training data across all classes. This theory is backed up by the decreasing performance when limiting to 1000 tweets per class, and the increasing accuracy when classifying negative tweets. 


\section{G2: Develop tools for visualising sentiment classified tweets}

We achieved goal 2 by implementing three different applications for visualising sentiment analysis data, and by designing and using a generic system for classifying tweets. 

We are satisfied with the way the generic system architecture is built. By just extending the Twitter API, no API documentation is needed, and if you are familiar with the Twitter API, you don't have to re-learn anything. In addition, if there is a system all ready integrated with Twitter data, the migration to our system is simple; swap the entry URL point from the Twitter API base URL to our system base URL. 

The current implementation of the API Layer is simplified in regards to authentication. The API Layer has no OAuth server, but rather uses its own credentials to connect to Twitter. This means, if 10 different users do 30 requests each, Twitter's request limit will engage and our API Layer will be put on hold until next window of requests. A better solution would have been to implement a mirror of Twitters OAuth server and pass on the received credentials to Twitter. This way each end-user or application client would have their own pool of requests. 

One important point when designing the generic system was to have it as fast as possible. This to be able to handle large amount of tweets when streaming or simply searching with a high count limit. In some cases a client can request 1000 tweets at once. This required a system that can operate in parallel and handle asynchronous connections. The way our system was built, the API layer works independent of the classification server and each request is parallel and asynchronous. This maximize the number of tweets we can handle and reduce the collected wait time for the client. This solution works great for the applications we have built and for up to 5 clients running simultaneously, but the system has not been tested with any more clients or stress-tested in any way. A stress-test might show a bottleneck or a weakness in the system that is not apparent at this point. 

\subsection{SentiMap}

The SentiMap application show interesting information and distribution of tweets a cross the USA. It is capable of handling at least 50 tweets per second, and updates the map's colour scheme for each tweet. Every tweet is grouped by state, so the system looses some information in regards to locality. It could have been interesting to add more details to the map, showing the exact origin of a Tweet in addition to it changing the state sentiment indication colour.

When opening SentiMap now, you start out from scratch. There is no history or storage for the tweets. If you were to open SentiMap and have it classify 1000 tweets, refreshing the application will remove these 1000 tweets. Adding a local storage for these tweets, could provide some usefulness. Also concatenating search data with stream data could be useful, starting the application with some initial data. If some big event happens now, a user has to be quick to open SentiMap to see the sentimental development. 

There is no automatic way of plugging in a different country to the SentiMap application, but the architecture allows for easy system extension. By having defined a clear MV* architecture, each module is fairly independent of each other, and can thus be replaced with different modules. To make this even easier, a plug-in system could have been designed and documented. 

By using WebSockets to provide data from the API Layer, a continuously connection is established. This means that if the API Server goes down, or the visualisation application back-end server restarts, the application will automatically reconnect without any user action. E.g. if you open SentiMap on a laptop, closes this laptop, and then re-open it, SentiMap will reconnect and start showing data again. This means you can have the client open over a long, long time, if necessary.   

\subsection{SentiGraph}
SentiGraph was developed as a light weight JavaScript application that runs entirely in the client's web browser. This makes the application both fast, and compatible with most platforms. It demonstrates how the API Layer (and the Twitter API) can be used to search for different topics or keywords. It would, however, be possible to combine this with a streaming service to append incoming tweets to the search result. But that would require a small server side application to push the streamed tweets to the client, and thus breaking the concept of having the entire application running in the browser.

SentiGraph is a good application for visualising the sentiment data that are generated by the sentiment classifier. But there are definitely room for more features, such as comparison of keywords and, of course, several different graphs and charts. A graph that showed changes in sentiment over time could be a useful tool to visualise trends, and possibly changes caused by certain events or happenings. 

\subsection{SentiStack}

SentiStack gives a good comparative view between several different Twitter search queries. Without defining any domain for the queries, the application can be used to compare anything, from products to ESC participation songs. This can also be it's down fall, as it might get too generic and not perform as well as an application specifically designed to do product comparisons. 

A big limitation of the SentiStack system is the number of requests to the Twitter API. The Twitter Search API has a limitation of the number of requests that is allowed each 15 minutes. SentiStack does two requests per query (to retrieve the total of 200 tweets). This means that if there are a total of 25 queries, 50 requests will be made each time "Compare" is pressed. Only 180 requests can be made every 15 minutes. So when having 25 queries, one can only update the comparative view 3 times every 15 minutes. To solve this problem, a more sophisticated OAuth solution is required on the API Layer. The API Layer needs to receive and pass along OAuth credentials to the Twitter API. This way the request limitations will be per end-user and not global amongst the users of the API Layer.

\subsubsection{Predicting ESC}

By looking at the results from predicting the winners of ESC, it is clear that it did not perform too well. Only 5 of the finalists predicted to be among the top 10 was correct. This is marginally better than the baseline of $3.85$, but not enough to make any conclusions. 

Ireland was picked as a clear winner by SentiStack, but in fact ended up in decidedly last place with only 5 points\footnote{\url{http://en.wikipedia.org/wiki/Eurovision_Song_Contest_2013}}. It is difficult to say why Ireland did so poorly when it shows a large amount of positive tweets. It could seem like the assumption of tweets being representative for the tele-voting on ESC is wrong. One important aspect is, that since Ireland is an English speaking country, their tweets is visible to the SenitStack system, unlike most of the other countries. This can lead to many bias tweets being registered from Ireland and thus skewing the results. 

SentiStack only fetches 200 tweets per query, this because of the limitation of number of requests from the Twitter API. This might be too few tweets to get a representative picture to predict or analyse statistical outcomes. A better solution might be to mine for data over a long period of time using the Stream API. 

It may seem as SentiStack is too generic to predict the outcome of a contest as complex as ESC. A more sophisticated application specifically designed to predict the outcome of ESC, could have used prior knowledge and expert opinions in addition to sentiment analysis, and accumulate opinion data over a larger period of time. ESC is not, as mentioned, entirely up to tele-voters, and relies on a panel of judges for 50\% of the votes\footnote{\url{http://en.wikipedia.org/wiki/Voting_at_the_Eurovision_Song_Contest}}, this has a large impact on the results.

%\textcolor{blue}{(8-10 pages) In this chapter you assess your results. Identify your contributions. Possible theory 
%building (establish cause-effect). Compare to other work described in chapter 2. Suggestions for improvements. Discuss 
%construct-, internal-, external- and conclusion-validity. The major challenge in this chapter is usually which axis you 
%want to structure your discussion around: research questions, contributions or studies. Find what works best for you 
%and your studies.}
%
%\textcolor{green}{Evaluation of research questions}
%
%\textcolor{blue}{If you did not answer these questions in the results chapter, now is the time to revisit.}
%
%\textcolor{green}{Evaluation of Contributions}
%
%\textcolor{blue}{How does our contributions fit with the state of the art we described in chapter 2? Do they extend the 
%field? In what way? How do your contributions compare to your research questions? Do you have your own reflections on 
%the contributions.}
%
%\textcolor{green}{Evaluation of Validity Threats}
%
%\textcolor{blue}{What are the major threats to our research? Mention the major threats like:}
%
%\begin{list}{$\bullet$}{}
%  \item Internal Validity 
%  \item External Validity
%  \item Construct Validity
%  \item Conclusion Validity
%\end{list}
%
%\textcolor{blue}{Note that you might have to discuss these separately for each study, and every validity might not be applicable depending on what research method you have used.}
%
%\textcolor{green}{Reflections on the research context}
%
%\textcolor{blue}{Optional. But it is often good to reflect on the (project) context of your research and how it has 
%affected you and your research.}